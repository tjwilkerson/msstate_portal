{"cells": [{"cell_type": "markdown", "id": "60b497af", "metadata": {"papermill": {"duration": 0.00368, "end_time": "2025-07-30T12:02:29.838514", "exception": false, "start_time": "2025-07-30T12:02:29.834834", "status": "completed"}, "tags": []}, "source": ["## Welcome to your notebook.\n"]}, {"cell_type": "markdown", "id": "70bd6929", "metadata": {"papermill": {"duration": 0.001995, "end_time": "2025-07-30T12:02:29.842839", "exception": false, "start_time": "2025-07-30T12:02:29.840844", "status": "completed"}, "tags": []}, "source": ["#### Run this cell to connect to your GIS and get started:"]}, {"cell_type": "code", "execution_count": 1, "id": "51d1f004", "metadata": {"execution": {"iopub.execute_input": "2025-07-30T12:02:29.848341Z", "iopub.status.busy": "2025-07-30T12:02:29.848012Z", "iopub.status.idle": "2025-07-30T12:04:13.751268Z", "shell.execute_reply": "2025-07-30T12:04:13.750521Z"}, "papermill": {"duration": 103.908881, "end_time": "2025-07-30T12:04:13.753638", "exception": false, "start_time": "2025-07-30T12:02:29.844757", "status": "completed"}, "tags": []}, "outputs": [], "source": ["from arcgis.gis import GIS\n", "gis = GIS(\"home\")"]}, {"cell_type": "code", "execution_count": 2, "id": "567c876d", "metadata": {"execution": {"iopub.execute_input": "2025-07-30T12:04:13.762788Z", "iopub.status.busy": "2025-07-30T12:04:13.762225Z", "iopub.status.idle": "2025-07-30T12:04:26.098098Z", "shell.execute_reply": "2025-07-30T12:04:26.097259Z"}, "papermill": {"duration": 12.342181, "end_time": "2025-07-30T12:04:26.099678", "exception": false, "start_time": "2025-07-30T12:04:13.757497", "status": "completed"}, "tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\r\n"]}, {"name": "stdout", "output_type": "stream", "text": ["\u001b[33mDEPRECATION: Loading egg at /opt/conda/lib/python3.11/site-packages/tflite_model_maker-0.3.4-py3.11.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\r\n", "\u001b[0m"]}], "source": ["!pip install earthengine-api geemap --q"]}, {"cell_type": "code", "execution_count": 3, "id": "cf00d888", "metadata": {"execution": {"iopub.execute_input": "2025-07-30T12:04:26.105710Z", "iopub.status.busy": "2025-07-30T12:04:26.105474Z", "iopub.status.idle": "2025-07-30T12:04:26.677180Z", "shell.execute_reply": "2025-07-30T12:04:26.676550Z"}, "papermill": {"duration": 0.576636, "end_time": "2025-07-30T12:04:26.678717", "exception": false, "start_time": "2025-07-30T12:04:26.102081", "status": "completed"}, "tags": []}, "outputs": [], "source": ["import ee\n", "import os\n", "import numpy as np\n", "import pandas as pd\n", "from datetime import datetime\n", "from datetime import timedelta\n", "from dataclasses import dataclass\n", "from concurrent.futures import ThreadPoolExecutor, as_completed\n", "from arcgis.features import FeatureLayerCollection"]}, {"cell_type": "code", "execution_count": 4, "id": "e5ced978", "metadata": {"execution": {"iopub.execute_input": "2025-07-30T12:04:26.686574Z", "iopub.status.busy": "2025-07-30T12:04:26.686026Z", "iopub.status.idle": "2025-07-30T12:04:27.243917Z", "shell.execute_reply": "2025-07-30T12:04:27.243317Z"}, "papermill": {"duration": 0.563211, "end_time": "2025-07-30T12:04:27.245279", "exception": false, "start_time": "2025-07-30T12:04:26.682068", "status": "completed"}, "tags": []}, "outputs": [], "source": ["# Replace with your actual values\n", "SERVICE_ACCOUNT = 'gee-daily-weather@sre-2025.iam.gserviceaccount.com'\n", "KEY_PATH = '/arcgis/home/data/key/sre-2025-2b6af5c825e3.json'\n", "\n", "# Authenticate and initialize Earth Engine\n", "credentials = ee.ServiceAccountCredentials(SERVICE_ACCOUNT, KEY_PATH)\n", "ee.Initialize(credentials)"]}, {"cell_type": "code", "execution_count": 5, "id": "5e2b4e8e", "metadata": {"execution": {"iopub.execute_input": "2025-07-30T12:04:27.252223Z", "iopub.status.busy": "2025-07-30T12:04:27.251990Z", "iopub.status.idle": "2025-07-30T12:04:27.257437Z", "shell.execute_reply": "2025-07-30T12:04:27.256854Z"}, "papermill": {"duration": 0.010546, "end_time": "2025-07-30T12:04:27.258766", "exception": false, "start_time": "2025-07-30T12:04:27.248220", "status": "completed"}, "tags": []}, "outputs": [], "source": ["# yesterday = (datetime.now() - timedelta(days=1)).date()\n", "\n", "@dataclass\n", "class Config:\n", "\n", "    grid_asset_path: str = \"projects/ee-thaimunhoz98/assets/MS_basin\" #location in my GEE Assets\n", "\n", "    # DataSource Collection names and configurations\n", "    precipitation_dataset = [\"precipitation\", \"RTMA\", \"NOAA/NWS/RTMA\", \"ACPC01\", \"hourly\"]\n", "    temperature_mean_dataset = [\"temperature_mean\", \"RTMA\", \"NOAA/NWS/RTMA\", \"TMP\", \"hourly\", \"celsius\"]\n", "    temperature_min_dataset = [\"temperature_min\", \"RTMA\", \"NOAA/NWS/RTMA\", \"TMP\", \"hourly\", \"celsius\"]\n", "    temperature_max_dataset = [\"temperature_max\", \"RTMA\", \"NOAA/NWS/RTMA\", \"TMP\", \"hourly\", \"celsius\"]\n", "    humidity_dataset = [\"humidity\", \"RTMA\", \"NOAA/NWS/RTMA\", \"SPFH\", \"hourly\"]\n", "    wind_dataset = [\"wind\", \"RTMA\", \"NOAA/NWS/RTMA\", \"WIND\", \"hourly\"]\n", "    solar_rad_dataset = [\"solar_rad\", \"GRIDMET\", \"IDAHO_EPSCOR/GRIDMET\", \"srad\", \"daily\"]\n", "    soil_moisture_dataset =[\"soil_moisture\", \"SMAP\", \"NASA/SMAP/SPL4SMGP/007\", \"sm_surface\", \"hourly\"]\n", "    evapo_dataset = [\"evapotranspiration\", \"SMAP\",\"NASA/SMAP/SPL4SMGP/007\", \"land_evapotranspiration_flux\", \"hourly\"]\n"]}, {"cell_type": "code", "execution_count": 6, "id": "9db94839", "metadata": {"execution": {"iopub.execute_input": "2025-07-30T12:04:27.263963Z", "iopub.status.busy": "2025-07-30T12:04:27.263752Z", "iopub.status.idle": "2025-07-30T12:04:27.271455Z", "shell.execute_reply": "2025-07-30T12:04:27.270821Z"}, "papermill": {"duration": 0.011853, "end_time": "2025-07-30T12:04:27.272718", "exception": false, "start_time": "2025-07-30T12:04:27.260865", "status": "completed"}, "tags": []}, "outputs": [], "source": ["def split_collection(feature_collection, batch_size):\n", "    ''' Split grid into more than one collection to handle GEE data limit'''\n", "    size = feature_collection.size().getInfo()\n", "    return [\n", "        feature_collection.toList(batch_size, i)\n", "        for i in range(0, size, batch_size)\n", "    ]\n", "\n", "def load_asset(asset_path):\n", "    '''Load the full grid from a GEE asset'''\n", "    grid_fc = ee.FeatureCollection(asset_path)\n", "    return grid_fc\n", "\n", "def celsius_to_fahrenheit(image, input_band, output_band=\"LST_F\"):\n", "    ''' Convert Celsius degrees temperature to Fahrenheit'''\n", "    if output_band is None:\n", "        output_band = input_band.replace(\"TMP\", \"LST_F\")\n", "    lst_f = image.expression(\n", "        \"(x * (9 / 5)) + 32\",\n", "        {\"x\": image.select(input_band)}\n", "    ).rename(output_band)\n", "    return image.addBands(lst_f)\n", "\n", "def kelvin_to_fahrenheit(image, input_band, output_band=\"LST_F\"):\n", "    lst_f = image.expression(\n", "        \"((x - 273.15) * (9 / 5)) + 32\",\n", "        {\n", "            \"x\": image.select(input_band)\n", "        }\n", "    ).rename(output_band)\n", "    return image.addBands(lst_f)\n", "\n", "def export_table(input_results, var_name, band_name, output_file):\n", "    ''' Export results from GEE as a table to feed the Dashboard'''\n", "    flat_data = input_results[0]\n", "    df = pd.json_normalize(flat_data)\n", "    try:\n", "        result_table = df.drop(columns=['geometry.type', 'geometry.coordinates']).rename(columns={band_name: var_name})\n", "    except:\n", "        print(result_table)\n", "        result_table = df.rename(columns={band_name: var_name})\n", "    #result_table = pd.DataFrame(input_results).rename(columns={band_name: var_name})\n", "    result_table['date_str'] = result_table['date'].dt.strftime('%m/%d/%Y')\n", "\n", "    result_table = result_table.drop_duplicates(subset=['TARGET_FID', 'date'], keep='last')\n", "\n", "    new_order = ['date', 'date_str', 'TARGET_FID', var_name]\n", "    result_table = result_table[new_order]\n", "    result_table.to_csv(output_file)\n", "\n", "def return_last_day(gpd_file_path):\n", "    ''' Read a shapefile and return the last day of available data'''\n", "    gdf = gpd.read_file(gpd_file_path)\n", "    return gdf.columns[-2]"]}, {"cell_type": "code", "execution_count": 7, "id": "44f927fc", "metadata": {"execution": {"iopub.execute_input": "2025-07-30T12:04:27.278030Z", "iopub.status.busy": "2025-07-30T12:04:27.277818Z", "iopub.status.idle": "2025-07-30T12:04:27.288868Z", "shell.execute_reply": "2025-07-30T12:04:27.288274Z"}, "papermill": {"duration": 0.015219, "end_time": "2025-07-30T12:04:27.290042", "exception": false, "start_time": "2025-07-30T12:04:27.274823", "status": "completed"}, "tags": []}, "outputs": [], "source": ["def run_historical(variable, grid_fc, start_date, end_date, collection_name, band_name, temporal_resolution, data_source, input_unit):\n", "\n", "    ''' Access data from collection and return a table related to each grid cell in the fishnet '''\n", "\n", "    date_list = pd.date_range(start=start_date, end=end_date, freq='D')\n", "\n", "    #grid_fc = toolbox.gdf2ee(grid_shapefile)\n", "\n", "    all_results = []\n", "\n", "    for i in np.arange(0, len(date_list) - 1):\n", "\n", "        if variable == \"precipitation\" or variable == \"evapotranspiration\":\n", "            if temporal_resolution == \"hourly\":\n", "                dataset = ee.ImageCollection(collection_name) \\\n", "                    .filterDate(date_list[0], date_list[1]) \\\n", "                    .select(band_name) \\\n", "                    .sum()\n", "            else:\n", "                dataset = ee.ImageCollection(collection_name) \\\n", "                    .filterDate(date_list[0], date_list[1]) \\\n", "                    .select(band_name) \\\n", "                    .first()\n", "\n", "        elif variable == \"temperature_min\":\n", "            dataset = ee.ImageCollection(collection_name) \\\n", "                    .filterDate(date_list[0], date_list[1]) \\\n", "                    .select(band_name) \\\n", "                    .min()\n", "        elif variable == \"temperature_max\":\n", "            dataset = ee.ImageCollection(collection_name) \\\n", "                    .filterDate(date_list[0], date_list[1]) \\\n", "                    .select(band_name) \\\n", "                    .max()\n", "        else:\n", "            if temporal_resolution == \"hourly\":\n", "                dataset = ee.ImageCollection(collection_name) \\\n", "                    .filterDate(date_list[0], date_list[1]) \\\n", "                    .select(band_name) \\\n", "                    .mean()\n", "            else:\n", "                dataset = ee.ImageCollection(collection_name) \\\n", "                    .filterDate(date_list[0], date_list[1]) \\\n", "                    .select(band_name) \\\n", "                    .first()\n", "        dataset = ee.Image(dataset)\n", "\n", "        # Check for null image\n", "        info = dataset.getInfo()\n", "        if not info or 'bands' not in info:\n", "            print(f\"[Error] Null image encountered for {variable} on {date_list[i]}. Skipping...\")\n", "            return None  # Signal the main loop to break or skip\n", "\n", "\n", "        if input_unit is not None:\n", "            if input_unit == \"celsius\":\n", "                dataset = celsius_to_fahrenheit(dataset, input_band=band_name).select(\"LST_F\")\n", "            elif input_unit == \"kelvin\":\n", "                dataset = kelvin_to_fahrenheit(dataset, input_band=band_name).select(\"LST_F\")\n", "\n", "        def make_compute_mean(dataset):\n", "            def compute_mean(feature):\n", "                mean = dataset.reduceRegion(\n", "                    reducer=ee.Reducer.mean(),\n", "                    geometry=feature.geometry(),\n", "                    scale=9000,\n", "                    maxPixels=1e9\n", "                )\n", "                return feature.set(mean)\n", "            return compute_mean\n", "\n", "        try:\n", "            compute_mean = make_compute_mean(dataset)\n", "            reduced = grid_fc.map(compute_mean).getInfo()\n", "\n", "            print(f\"Starting export for date {date_list[i]}\")\n", "            for feat in reduced['features']:\n", "                props = feat['properties']\n", "                props['date'] = date_list[i]\n", "                props['geometry'] = feat['geometry']\n", "                all_results.append(props)\n", "\n", "        except Exception as e:\n", "            print(f\"Temp [Warning] Date {date_list[i]} failed on full grid_fc: {e}\")\n", "            grid_batches = split_collection(grid_fc, batch_size=500)\n", "\n", "            def process_batch(batch):\n", "                try:\n", "                    compute_mean = make_compute_mean(dataset)\n", "                    grid_fc_batch = ee.FeatureCollection(batch)\n", "                    reduced = grid_fc_batch.map(compute_mean).getInfo()\n", "\n", "                    results = []\n", "                    if reduced and 'features' in reduced:\n", "                        for feat in reduced['features']:\n", "                            props = feat['properties']\n", "                            props['date'] = date_list[i]\n", "                            props['geometry'] = feat['geometry']\n", "                            results.append(props)\n", "                    return results\n", "                except Exception as err:\n", "                    print(f\"[Error] Failed batch for date {date_list[i]}: {err}\")\n", "\n", "            with ThreadPoolExecutor(max_workers=5) as executor:\n", "                futures = [executor.submit(process_batch, batch) for batch in grid_batches]\n", "                for future in as_completed(futures):\n", "                    try:\n", "                        result = future.result()\n", "                        all_results.extend(result)\n", "                    except:\n", "                        break\n", "\n", "        for item in all_results:\n", "            if \"LST_F\" in item and variable not in item:\n", "                item[variable] = item.pop(\"LST_F\")\n", "\n", "    return all_results"]}, {"cell_type": "code", "execution_count": 8, "id": "4fd30877", "metadata": {"execution": {"iopub.execute_input": "2025-07-30T12:04:27.295454Z", "iopub.status.busy": "2025-07-30T12:04:27.295225Z", "iopub.status.idle": "2025-07-30T12:26:56.836729Z", "shell.execute_reply": "2025-07-30T12:26:56.836021Z"}, "papermill": {"duration": 1349.545828, "end_time": "2025-07-30T12:26:56.837984", "exception": false, "start_time": "2025-07-30T12:04:27.292156", "status": "completed"}, "tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Running from 2025-07-27 to 2025-07-29\n", "\n", "=== Processing 2025-07-27 ===\n", "Running solar_rad for 2025-07-27\n"]}, {"name": "stdout", "output_type": "stream", "text": ["Temp [Warning] Date 2025-07-27 00:00:00 failed on full grid_fc: Collection query aborted after accumulating over 5000 elements.\n"]}, {"name": "stdout", "output_type": "stream", "text": ["Running humidity for 2025-07-27\n"]}, {"name": "stdout", "output_type": "stream", "text": ["Temp [Warning] Date 2025-07-27 00:00:00 failed on full grid_fc: Collection query aborted after accumulating over 5000 elements.\n"]}, {"name": "stdout", "output_type": "stream", "text": ["Running wind for 2025-07-27\n"]}, {"name": "stdout", "output_type": "stream", "text": ["Temp [Warning] Date 2025-07-27 00:00:00 failed on full grid_fc: Collection query aborted after accumulating over 5000 elements.\n"]}, {"name": "stdout", "output_type": "stream", "text": ["Running precipitation for 2025-07-27\n"]}, {"name": "stdout", "output_type": "stream", "text": ["Temp [Warning] Date 2025-07-27 00:00:00 failed on full grid_fc: Collection query aborted after accumulating over 5000 elements.\n"]}, {"name": "stdout", "output_type": "stream", "text": ["Running temperature_max for 2025-07-27\n"]}, {"name": "stdout", "output_type": "stream", "text": ["Temp [Warning] Date 2025-07-27 00:00:00 failed on full grid_fc: Collection query aborted after accumulating over 5000 elements.\n"]}, {"name": "stdout", "output_type": "stream", "text": ["Running temperature_min for 2025-07-27\n"]}, {"name": "stdout", "output_type": "stream", "text": ["Temp [Warning] Date 2025-07-27 00:00:00 failed on full grid_fc: Collection query aborted after accumulating over 5000 elements.\n"]}, {"name": "stdout", "output_type": "stream", "text": ["Running temperature_mean for 2025-07-27\n"]}, {"name": "stdout", "output_type": "stream", "text": ["Temp [Warning] Date 2025-07-27 00:00:00 failed on full grid_fc: Collection query aborted after accumulating over 5000 elements.\n"]}, {"name": "stdout", "output_type": "stream", "text": ["[Success] combined Uploaded batch 1/83 on attempt 1\n"]}, {"name": "stdout", "output_type": "stream", "text": ["[Success] combined Uploaded batch 2/83 on attempt 1\n", "[Success] combined Uploaded batch 5/83 on attempt 1\n", "[Success] combined Uploaded batch 3/83 on attempt 1\n", "[Success] combined Uploaded batch 4/83 on attempt 1\n"]}, {"name": "stdout", "output_type": "stream", "text": ["[Success] combined Uploaded batch 6/83 on attempt 1\n", "[Success] combined Uploaded batch 9/83 on attempt 1\n", "[Success] combined Uploaded batch 8/83 on attempt 1\n"]}, {"name": "stdout", "output_type": "stream", "text": ["[Success] combined Uploaded batch 10/83 on attempt 1\n", "[Success] combined Uploaded batch 7/83 on attempt 1\n"]}, {"name": "stdout", "output_type": "stream", "text": ["[Success] combined Uploaded batch 11/83 on attempt 1\n"]}, {"name": "stdout", "output_type": "stream", "text": ["[Success] combined Uploaded batch 13/83 on attempt 1\n", "[Success] combined Uploaded batch 14/83 on attempt 1\n", "[Success] combined Uploaded batch 12/83 on attempt 1\n"]}, {"name": "stdout", "output_type": "stream", "text": ["[Success] combined Uploaded batch 15/83 on attempt 1\n"]}, {"name": "stdout", "output_type": "stream", "text": ["[Success] combined Uploaded batch 17/83 on attempt 1\n", "[Success] combined Uploaded batch 16/83 on attempt 1\n", "[Success] combined Uploaded batch 18/83 on attempt 1\n", "[Success] combined Uploaded batch 19/83 on attempt 1\n"]}, {"name": "stdout", "output_type": "stream", "text": ["[Success] combined Uploaded batch 20/83 on attempt 1\n"]}, {"name": "stdout", "output_type": "stream", "text": ["[Success] combined Uploaded batch 24/83 on attempt 1\n", "[Success] combined Uploaded batch 21/83 on attempt 1\n"]}, {"name": "stdout", "output_type": "stream", "text": ["[Success] combined Uploaded batch 23/83 on attempt 1\n", "[Success] combined Uploaded batch 22/83 on attempt 1\n"]}, {"name": "stdout", "output_type": "stream", "text": ["[Success] combined Uploaded batch 25/83 on attempt 1\n"]}, {"name": "stdout", "output_type": "stream", "text": ["[Success] combined Uploaded batch 27/83 on attempt 1\n", "[Success] combined Uploaded batch 26/83 on attempt 1\n", "[Success] combined Uploaded batch 29/83 on attempt 1\n"]}, {"name": "stdout", "output_type": "stream", "text": ["[Success] combined Uploaded batch 28/83 on attempt 1\n"]}, {"name": "stdout", "output_type": "stream", "text": ["[Success] combined Uploaded batch 30/83 on attempt 1\n", "[Success] combined Uploaded batch 31/83 on attempt 1\n"]}, {"name": "stdout", "output_type": "stream", "text": ["[Success] combined Uploaded batch 33/83 on attempt 1\n", "[Success] combined Uploaded batch 34/83 on attempt 1\n", "[Success] combined Uploaded batch 32/83 on attempt 1\n"]}, {"name": "stdout", "output_type": "stream", "text": ["[Success] combined Uploaded batch 35/83 on attempt 1\n"]}, {"name": "stdout", "output_type": "stream", "text": ["[Success] combined Uploaded batch 36/83 on attempt 1\n", "[Success] combined Uploaded batch 37/83 on attempt 1\n"]}, {"name": "stdout", "output_type": "stream", "text": ["[Success] combined Uploaded batch 38/83 on attempt 1\n", "[Success] combined Uploaded batch 39/83 on attempt 1\n"]}, {"name": "stdout", "output_type": "stream", "text": ["[Success] combined Uploaded batch 40/83 on attempt 1\n"]}, {"name": "stdout", "output_type": "stream", "text": ["[Success] combined Uploaded batch 42/83 on attempt 1\n", "[Success] combined Uploaded batch 41/83 on attempt 1\n"]}, {"name": "stdout", "output_type": "stream", "text": ["[Success] combined Uploaded batch 43/83 on attempt 1\n", "[Success] combined Uploaded batch 44/83 on attempt 1\n"]}, {"name": "stdout", "output_type": "stream", "text": ["[Success] combined Uploaded batch 45/83 on attempt 1\n", "[Success] combined Uploaded batch 46/83 on attempt 1\n", "[Success] combined Uploaded batch 47/83 on attempt 1\n"]}, {"name": "stdout", "output_type": "stream", "text": ["[Success] combined Uploaded batch 48/83 on attempt 1\n", "[Success] combined Uploaded batch 49/83 on attempt 1\n"]}, {"name": "stdout", "output_type": "stream", "text": ["[Success] combined Uploaded batch 52/83 on attempt 1\n", "[Success] combined Uploaded batch 50/83 on attempt 1\n", "[Success] combined Uploaded batch 53/83 on attempt 1\n"]}, {"name": "stdout", "output_type": "stream", "text": ["[Success] combined Uploaded batch 51/83 on attempt 1\n"]}, {"name": "stdout", "output_type": "stream", "text": ["[Success] combined Uploaded batch 54/83 on attempt 1\n"]}, {"name": "stdout", "output_type": "stream", "text": ["[Success] combined Uploaded batch 55/83 on attempt 1\n", "[Success] combined Uploaded batch 56/83 on attempt 1\n", "[Success] combined Uploaded batch 57/83 on attempt 1\n"]}, {"name": "stdout", "output_type": "stream", "text": ["[Success] combined Uploaded batch 58/83 on attempt 1\n"]}, {"name": "stdout", "output_type": "stream", "text": ["[Success] combined Uploaded batch 59/83 on attempt 1\n"]}, {"name": "stdout", "output_type": "stream", "text": ["[Success] combined Uploaded batch 60/83 on attempt 1\n", "[Success] combined Uploaded batch 61/83 on attempt 1\n"]}, {"name": "stdout", "output_type": "stream", "text": ["[Success] combined Uploaded batch 62/83 on attempt 1\n"]}, {"name": "stdout", "output_type": "stream", "text": ["[Success] combined Uploaded batch 63/83 on attempt 1\n"]}, {"name": "stdout", "output_type": "stream", "text": ["[Success] combined Uploaded batch 65/83 on attempt 1\n"]}, {"name": "stdout", "output_type": "stream", "text": ["[Success] combined Uploaded batch 66/83 on attempt 1\n"]}, {"name": "stdout", "output_type": "stream", "text": ["[Success] combined Uploaded batch 64/83 on attempt 1\n", "[Success] combined Uploaded batch 67/83 on attempt 1\n"]}, {"name": "stdout", "output_type": "stream", "text": ["[Success] combined Uploaded batch 68/83 on attempt 1\n"]}, {"name": "stdout", "output_type": "stream", "text": ["[Success] combined Uploaded batch 69/83 on attempt 1\n", "[Success] combined Uploaded batch 70/83 on attempt 1\n"]}, {"name": "stdout", "output_type": "stream", "text": ["[Success] combined Uploaded batch 73/83 on attempt 1\n", "[Success] combined Uploaded batch 72/83 on attempt 1\n", "[Success] combined Uploaded batch 71/83 on attempt 1\n"]}, {"name": "stdout", "output_type": "stream", "text": ["[Success] combined Uploaded batch 74/83 on attempt 1\n", "[Success] combined Uploaded batch 75/83 on attempt 1\n"]}, {"name": "stdout", "output_type": "stream", "text": ["[Success] combined Uploaded batch 78/83 on attempt 1\n", "[Success] combined Uploaded batch 76/83 on attempt 1\n"]}, {"name": "stdout", "output_type": "stream", "text": ["[Success] combined Uploaded batch 77/83 on attempt 1\n"]}, {"name": "stdout", "output_type": "stream", "text": ["[Success] combined Uploaded batch 80/83 on attempt 1\n"]}, {"name": "stdout", "output_type": "stream", "text": ["[Success] combined Uploaded batch 79/83 on attempt 1\n", "[Success] combined Uploaded batch 81/83 on attempt 1\n", "[Success] combined Uploaded batch 82/83 on attempt 1\n"]}, {"name": "stdout", "output_type": "stream", "text": ["[Success] combined Uploaded batch 83/83 on attempt 1\n"]}, {"name": "stdout", "output_type": "stream", "text": ["\u2705\u2705\u2705\u2705\u2705 Appended 2025-07-27 to /arcgis/home/data/date_check/last_run_bulk.csv\n", "\u23f1\ufe0f  Elapsed time for 2025-07-27: 1349.02 seconds\n", "\n", "=== Processing 2025-07-28 ===\n", "Running solar_rad for 2025-07-28\n"]}, {"name": "stdout", "output_type": "stream", "text": ["[Error] Null image encountered for solar_rad on 2025-07-28 00:00:00. Skipping...\n", "Skipping day 2025-07-28 due to null image.\n", "\u23ed\ufe0f  Skipped 2025-07-28 due to null input data.\n", "\n", "=== Processing 2025-07-29 ===\n", "Running solar_rad for 2025-07-29\n", "[Error] Null image encountered for solar_rad on 2025-07-29 00:00:00. Skipping...\n", "Skipping day 2025-07-29 due to null image.\n", "\u23ed\ufe0f  Skipped 2025-07-29 due to null input data.\n"]}], "source": ["# === CONFIG & DATE SETUP ===\n", "config = Config()\n", "tracking_file = \"/arcgis/home/data/date_check/last_run_bulk.csv\"\n", "\n", "# Load the last run date\n", "if os.path.exists(tracking_file):\n", "    df_out = pd.read_csv(tracking_file)\n", "    df_out['last_run_date'] = pd.to_datetime(df_out['last_run_date'])\n", "    latest_date = df_out['last_run_date'].max().date()\n", "    start_date = latest_date + timedelta(days=1)\n", "else:\n", "    # First run: manually define the start date\n", "    start_date = pd.to_datetime(\"2023-02-03\").date()\n", "\n", "# Manually define the end date (not inclusive)\n", "end_date = pd.to_datetime(pd.Timestamp.today()).date()\n", "\n", "print(f\"Running from {start_date} to {end_date - timedelta(days=1)}\")\n", "\n", "# === DATASETS & GRID SETUP ===\n", "datasets = [\n", "    (*config.solar_rad_dataset, None),\n", "    (*config.humidity_dataset, None),\n", "    (*config.wind_dataset, None),\n", "    (*config.precipitation_dataset, None),\n", "    config.temperature_max_dataset,\n", "    config.temperature_min_dataset,\n", "    config.temperature_mean_dataset\n", "]\n", "\n", "grid_fc = load_asset(config.grid_asset_path)\n", "\n", "# === DAILY LOOP ===\n", "current = start_date\n", "while current < end_date:\n", "    print(f\"\\n=== Processing {current} ===\")\n", "    time1 = time.time()\n", "\n", "    skip_day = False\n", "\n", "    for variable, data_source, collection_name, band_name, temporal_resolution, input_unit in datasets:\n", "        print(f\"Running {variable} for {current}\")\n", "        result = run_historical(\n", "            variable=variable,\n", "            grid_fc=grid_fc,\n", "            start_date=current,\n", "            end_date=current + timedelta(days=1),\n", "            collection_name=collection_name,\n", "            band_name=band_name,\n", "            temporal_resolution=temporal_resolution,\n", "            data_source=data_source,\n", "            input_unit=input_unit\n", "        )\n", "\n", "        if result is None:\n", "            print(f\"Skipping day {current} due to null image.\")\n", "            skip_day = True\n", "            break  # Skip the rest of the variables\n", "\n", "        output_table = f\"/arcgis/home/data/first_day/{variable}.csv\"\n", "        try:\n", "            export_table([result], variable, band_name, output_table)\n", "        except Exception as e:\n", "            print(f\"[Error] Failed to export {variable} on {current}: {e}\")\n", "            skip_day = True\n", "            break\n", "\n", "    # If we successfully ran all variables, merge and upload\n", "    if not skip_day:\n", "        # === MERGE ===\n", "        variables = [\n", "            \"humidity\", \"precipitation\", \"solar_rad\",\n", "            \"temperature_max\", \"temperature_mean\", \"temperature_min\", \"wind\"\n", "        ]\n", "        base_path = \"/arcgis/home/data/first_day\"\n", "        merged_df = pd.read_csv(os.path.join(base_path, f\"{variables[0]}.csv\"))\n", "        for var in variables[1:]:\n", "            df = pd.read_csv(os.path.join(base_path, f\"{var}.csv\"))\n", "            merged_df = pd.merge(merged_df, df, on=\"TARGET_FID\", how=\"outer\", suffixes=('', '_dup'))\n", "            merged_df = merged_df.loc[:, ~merged_df.columns.str.endswith('_dup')]\n", "        merged_df.drop(columns=['Unnamed: 0', 'date_str'], errors='ignore', inplace=True)\n", "        merged_df.to_csv(\"/arcgis/home/data/first_day/merged_output.csv\", index=False)\n", "    \n", "        # === PUSH TO FEATURE TABLE ===\n", "        item_id = \"e7c7151adfd549239c31f9cc2363831c\"\n", "        item = gis.content.get(item_id)\n", "        flc = FeatureLayerCollection.fromitem(item)\n", "        related_table_combined = flc.tables[0]\n", "    \n", "        df_combined = pd.read_csv(\"/arcgis/home/data/first_day/merged_output.csv\")\n", "        df_combined['date'] = pd.to_datetime(df_combined['date'], errors='coerce')\n", "        df_combined['date'] = df_combined['date'].apply(lambda dt: dt.replace(hour=8) if pd.notnull(dt) else dt)\n", "    \n", "        features_combined = [{\"attributes\": rec} for rec in df_combined.to_dict(orient='records')]\n", "    \n", "        def parallel_batch_edit_features(layer, features, label=\"\", batch_size=500, max_workers=5, max_retries=3):\n", "            batches = [features[i:i + batch_size] for i in range(0, len(features), batch_size)]\n", "            def upload_batch(batch_index, batch, label):\n", "                for attempt in range(1, max_retries + 1):\n", "                    try:\n", "                        result = layer.edit_features(adds=batch)\n", "                        if result.get(\"addResults\", []):\n", "                            print(f\"[Success] {label} Uploaded batch {batch_index + 1}/{len(batches)} on attempt {attempt}\")\n", "                            return result\n", "                        else:\n", "                            raise Exception(f\"No addResults: {result}\")\n", "                    except Exception as e:\n", "                        print(f\"[Retry {attempt}] {label} Batch {batch_index + 1} failed: {e}\")\n", "                        time.sleep(2)\n", "                print(f\"[Failed] {label} Batch {batch_index + 1} permanently failed.\")\n", "                return None\n", "            with ThreadPoolExecutor(max_workers=max_workers) as executor:\n", "                futures = [executor.submit(upload_batch, i, b, label) for i, b in enumerate(batches)]\n", "                for f in as_completed(futures): f.result()\n", "    \n", "        parallel_batch_edit_features(related_table_combined, features_combined, label=\"combined\")\n", "    \n", "        # === UPDATE DATE TRACKER ===\n", "        last_date = df_combined['date'].max()\n", "    \n", "        if os.path.exists(tracking_file):\n", "            df_out = pd.read_csv(tracking_file)\n", "            df_out['last_run_date'] = pd.to_datetime(df_out['last_run_date'])\n", "            if last_date not in df_out['last_run_date'].values:\n", "                df_out = pd.concat([df_out, pd.DataFrame({'last_run_date': [last_date]})], ignore_index=True)\n", "        else:\n", "            df_out = pd.DataFrame({'last_run_date': [last_date]})\n", "    \n", "        df_out.sort_values('last_run_date', inplace=True)\n", "        df_out.to_csv(tracking_file, index=False)\n", "    \n", "        print(f\"\u2705\u2705\u2705\u2705\u2705 Appended {last_date.date()} to {tracking_file}\")\n", "        print(f\"\u23f1\ufe0f  Elapsed time for {current}: {time.time() - time1:.2f} seconds\")\n", "    else:\n", "        print(f\"\u23ed\ufe0f  Skipped {current} due to null input data.\")\n", "    \n", "    # Always increment date\n", "    current += timedelta(days=1)\n"]}, {"cell_type": "code", "execution_count": null, "id": "58bcda6a", "metadata": {"papermill": {"duration": 0.004469, "end_time": "2025-07-30T12:26:56.846981", "exception": false, "start_time": "2025-07-30T12:26:56.842512", "status": "completed"}, "tags": []}, "outputs": [], "source": []}], "metadata": {"esriNotebookRuntime": {"notebookRuntimeName": "ArcGIS Notebook Python 3 Advanced", "notebookRuntimeVersion": "12.0"}, "kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.11.11"}, "papermill": {"default_parameters": {}, "duration": 1475.654655, "end_time": "2025-07-30T12:26:59.701894", "environment_variables": {}, "exception": null, "input_path": "/arcgis/home/.tasks/a8a3ce5d8a634b90a484977159661793/8960c0e3334448a0b3e7160d0ce4d8cd.ipynb", "output_path": "/arcgis/home/.tasks/a8a3ce5d8a634b90a484977159661793/output.ipynb", "start_time": "2025-07-30T12:02:24.047239", "version": "2.6.0"}}, "nbformat": 4, "nbformat_minor": 5}